{"cells":[{"cell_type":"markdown","metadata":{"id":"gZedzwhq0Z43"},"source":["# PairSpec-dLLM 評価ノートブック\n","\n","Google Drive 上に配置済みの Fast-dLLM/PairSpec-dLLM を直接参照し、\n","PairSpec（先行ドラフト併走）あり/なしの生成性能を **GSM8K (openai/gsm8k)** と **HumanEval (openai/openai_humaneval)** で比較します。\n","\n","- 速度系指標: latency / tokens per second / forward passes (NFE)\n","- 精度系指標: GSM8K Accuracy, HumanEval Pass@1\n","- モデルは Drive 上にダウンロード済みのものを直接使用（ローカル複製なし）\n","- PairSpec 有効時は draft モデルを別 GPU/同 GPU 上で並列起動します"],"id":"gZedzwhq0Z43"},{"cell_type":"markdown","metadata":{"id":"zp4CLMPF0Z44"},"source":["## 0. 手順概要\n","1. Google Drive をマウントし、既存の `PairSpec-dLLM` ルートへ `os.chdir`。\n","2. 依存関係をインストールし、（必要なら）Hugging Face でログイン。\n","3. 評価用の共通設定（モデルパス・ブロック長・サンプル数など）を記入。\n","4. ノートブック内の関数でベースライン / PairSpec の両方を実行。\n","5. 指標サマリとサンプルごとの詳細ログを確認。"],"id":"zp4CLMPF0Z44"},{"cell_type":"markdown","metadata":{"id":"YfzQFQE70Z44"},"source":["## 1. Google Drive をマウント（※既にマウント済みならスキップ可）"],"id":"YfzQFQE70Z44"},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4EsBSLvI0Z45","executionInfo":{"status":"ok","timestamp":1761537575275,"user_tz":-540,"elapsed":17361,"user":{"displayName":"俊輔","userId":"03232639100731679356"}},"outputId":"1b650fd4-6f33-4d95-9f68-38ccf29eda16"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"4EsBSLvI0Z45"},{"cell_type":"markdown","metadata":{"id":"ZHSuvWAc0Z45"},"source":["## 2. 作業ディレクトリと環境変数のセット\n","- `DRIVE_PROJECT_DIR` を Drive 上の `PairSpec-dLLM` ルートに変更してください。\n","- 既存ファイルをそのまま参照するため、ローカルへの rsync/コピーは行いません。"],"id":"ZHSuvWAc0Z45"},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vUhHu1PK0Z45","executionInfo":{"status":"ok","timestamp":1761537576947,"user_tz":-540,"elapsed":166,"user":{"displayName":"俊輔","userId":"03232639100731679356"}},"outputId":"978b067e-d108-4daa-c8f4-9243750837a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Working directory: /content/drive/MyDrive/PairSpec-dLLM\n"]}],"source":["import os, sys\n","\n","DRIVE_PROJECT_DIR = '/content/drive/MyDrive/PairSpec-dLLM'  # ★必要に応じて変更\n","\n","if not os.path.isdir(DRIVE_PROJECT_DIR):\n","    raise FileNotFoundError(f'{DRIVE_PROJECT_DIR} が見つかりません。パスを確認してください。')\n","\n","os.chdir(DRIVE_PROJECT_DIR)\n","if DRIVE_PROJECT_DIR not in sys.path:\n","    sys.path.insert(0, DRIVE_PROJECT_DIR)\n","\n","os.environ['HF_ALLOW_CODE_EVAL'] = '1'\n","os.environ['HF_DATASETS_TRUST_REMOTE_CODE'] = '1'\n","print('Working directory:', os.getcwd())"],"id":"vUhHu1PK0Z45"},{"cell_type":"markdown","metadata":{"id":"3upFEKzY0Z45"},"source":["## 3. 依存パッケージのインストール\n","- プロジェクト付属の `requirements.txt` に加え、評価で用いる `datasets` / `evaluate` / `accelerate` を最新化します。"],"id":"3upFEKzY0Z45"},{"cell_type":"code","execution_count":3,"id":"b4ab7288","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b4ab7288","executionInfo":{"status":"ok","timestamp":1761537625136,"user_tz":-540,"elapsed":47068,"user":{"displayName":"俊輔","userId":"03232639100731679356"}},"outputId":"1486493e-0c3d-4976-caa2-a454ddd1c0ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n","Collecting pip\n","  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n","Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pip\n","  Attempting uninstall: pip\n","    Found existing installation: pip 24.1.2\n","    Uninstalling pip-24.1.2:\n","      Successfully uninstalled pip-24.1.2\n","Successfully installed pip-25.3\n","Collecting transformers==4.49.0 (from -r requirements.txt (line 1))\n","  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n","Collecting lm_eval==0.4.8 (from -r requirements.txt (line 2))\n","  Downloading lm_eval-0.4.8-py3-none-any.whl.metadata (50 kB)\n","Collecting accelerate==0.34.2 (from -r requirements.txt (line 3))\n","  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n","Collecting antlr4-python3-runtime==4.11 (from -r requirements.txt (line 4))\n","  Downloading antlr4_python3_runtime-4.11.0-py3-none-any.whl.metadata (291 bytes)\n","Collecting math_verify (from -r requirements.txt (line 5))\n","  Downloading math_verify-0.8.0-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (1.13.3)\n","Requirement already satisfied: hf_xet in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (1.1.10)\n","Collecting pydantic==2.10.6 (from -r requirements.txt (line 8))\n","  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n","Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (5.49.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0->-r requirements.txt (line 1)) (3.20.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0->-r requirements.txt (line 1)) (0.35.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0->-r requirements.txt (line 1)) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0->-r requirements.txt (line 1)) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0->-r requirements.txt (line 1)) (6.0.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0->-r requirements.txt (line 1)) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0->-r requirements.txt (line 1)) (2.32.4)\n","Collecting tokenizers<0.22,>=0.21 (from transformers==4.49.0->-r requirements.txt (line 1))\n","  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0->-r requirements.txt (line 1)) (0.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.49.0->-r requirements.txt (line 1)) (4.67.1)\n","Collecting evaluate (from lm_eval==0.4.8->-r requirements.txt (line 2))\n","  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n","Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.8->-r requirements.txt (line 2)) (4.0.0)\n","Collecting jsonlines (from lm_eval==0.4.8->-r requirements.txt (line 2))\n","  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: numexpr in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.8->-r requirements.txt (line 2)) (2.14.1)\n","Requirement already satisfied: peft>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.8->-r requirements.txt (line 2)) (0.17.1)\n","Collecting pybind11>=2.6.2 (from lm_eval==0.4.8->-r requirements.txt (line 2))\n","  Downloading pybind11-3.0.1-py3-none-any.whl.metadata (10.0 kB)\n","Collecting pytablewriter (from lm_eval==0.4.8->-r requirements.txt (line 2))\n","  Downloading pytablewriter-1.2.1-py3-none-any.whl.metadata (38 kB)\n","Collecting rouge-score>=0.0.4 (from lm_eval==0.4.8->-r requirements.txt (line 2))\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting sacrebleu>=1.5.0 (from lm_eval==0.4.8->-r requirements.txt (line 2))\n","  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n","Requirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.8->-r requirements.txt (line 2)) (1.6.1)\n","Collecting sqlitedict (from lm_eval==0.4.8->-r requirements.txt (line 2))\n","  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.8->-r requirements.txt (line 2)) (2.8.0+cu126)\n","Collecting tqdm-multiprocess (from lm_eval==0.4.8->-r requirements.txt (line 2))\n","  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: zstandard in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.8->-r requirements.txt (line 2)) (0.25.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.8->-r requirements.txt (line 2)) (0.3.8)\n","Collecting word2number (from lm_eval==0.4.8->-r requirements.txt (line 2))\n","  Downloading word2number-1.1.zip (9.7 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: more_itertools in /usr/local/lib/python3.12/dist-packages (from lm_eval==0.4.8->-r requirements.txt (line 2)) (10.8.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate==0.34.2->-r requirements.txt (line 3)) (5.9.5)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic==2.10.6->-r requirements.txt (line 8)) (0.7.0)\n","Collecting pydantic-core==2.27.2 (from pydantic==2.10.6->-r requirements.txt (line 8))\n","  Downloading pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic==2.10.6->-r requirements.txt (line 8)) (4.15.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.49.0->-r requirements.txt (line 1)) (2025.3.0)\n","Collecting latex2sympy2_extended==1.10.2 (from math_verify->-r requirements.txt (line 5))\n","  Downloading latex2sympy2_extended-1.10.2-py3-none-any.whl.metadata (5.3 kB)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->-r requirements.txt (line 6)) (1.3.0)\n","Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 9)) (24.1.0)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 9)) (4.11.0)\n","Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 9)) (1.1.0)\n","Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 9)) (0.119.1)\n","Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 9)) (0.6.3)\n","Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 9)) (1.13.3)\n","Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 9)) (0.1.2)\n","Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 9)) (0.28.1)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 9)) (3.1.6)\n","Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 9)) (3.0.3)\n","Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 9)) (3.11.3)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 9)) (2.2.2)\n","Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 9)) (11.3.0)\n","Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 9)) (0.25.1)\n","Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 9)) (0.0.20)\n","Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 9)) (0.14.1)\n","Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 9)) (0.1.6)\n","Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 9)) (2.10.0)\n","Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 9)) (0.48.0)\n","Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 9)) (0.13.3)\n","Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 9)) (0.20.0)\n","Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r requirements.txt (line 9)) (0.38.0)\n","Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio->-r requirements.txt (line 9)) (15.0.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio->-r requirements.txt (line 9)) (3.11)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio->-r requirements.txt (line 9)) (1.3.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio->-r requirements.txt (line 9)) (2025.10.5)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio->-r requirements.txt (line 9)) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio->-r requirements.txt (line 9)) (0.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 9)) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 9)) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 9)) (2025.2)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio->-r requirements.txt (line 9)) (8.3.0)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio->-r requirements.txt (line 9)) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio->-r requirements.txt (line 9)) (13.9.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.16.0->lm_eval==0.4.8->-r requirements.txt (line 2)) (18.1.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.16.0->lm_eval==0.4.8->-r requirements.txt (line 2)) (3.6.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.16.0->lm_eval==0.4.8->-r requirements.txt (line 2)) (0.70.16)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.8->-r requirements.txt (line 2)) (3.13.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.8->-r requirements.txt (line 2)) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.8->-r requirements.txt (line 2)) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.8->-r requirements.txt (line 2)) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.8->-r requirements.txt (line 2)) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.8->-r requirements.txt (line 2)) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.8->-r requirements.txt (line 2)) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.8->-r requirements.txt (line 2)) (1.22.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio->-r requirements.txt (line 9)) (1.17.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.49.0->-r requirements.txt (line 1)) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.49.0->-r requirements.txt (line 1)) (2.5.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio->-r requirements.txt (line 9)) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio->-r requirements.txt (line 9)) (2.19.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio->-r requirements.txt (line 9)) (0.1.2)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.8->-r requirements.txt (line 2)) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.8->-r requirements.txt (line 2)) (3.9.1)\n","Collecting portalocker (from sacrebleu>=1.5.0->lm_eval==0.4.8->-r requirements.txt (line 2))\n","  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.8->-r requirements.txt (line 2)) (0.9.0)\n","Collecting colorama (from sacrebleu>=1.5.0->lm_eval==0.4.8->-r requirements.txt (line 2))\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.8->-r requirements.txt (line 2)) (5.4.0)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.8->-r requirements.txt (line 2)) (1.16.2)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.8->-r requirements.txt (line 2)) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.8->-r requirements.txt (line 2)) (3.6.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.8->-r requirements.txt (line 2)) (75.2.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.8->-r requirements.txt (line 2)) (3.5)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.8->-r requirements.txt (line 2)) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.8->-r requirements.txt (line 2)) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.8->-r requirements.txt (line 2)) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.8->-r requirements.txt (line 2)) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.8->-r requirements.txt (line 2)) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.8->-r requirements.txt (line 2)) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.8->-r requirements.txt (line 2)) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.8->-r requirements.txt (line 2)) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.8->-r requirements.txt (line 2)) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.8->-r requirements.txt (line 2)) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.8->-r requirements.txt (line 2)) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.8->-r requirements.txt (line 2)) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.8->-r requirements.txt (line 2)) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.8->-r requirements.txt (line 2)) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->lm_eval==0.4.8->-r requirements.txt (line 2)) (3.4.0)\n","Collecting DataProperty<2,>=1.1.0 (from pytablewriter->lm_eval==0.4.8->-r requirements.txt (line 2))\n","  Downloading DataProperty-1.1.0-py3-none-any.whl.metadata (11 kB)\n","Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm_eval==0.4.8->-r requirements.txt (line 2))\n","  Downloading mbstrdecoder-1.1.4-py3-none-any.whl.metadata (4.3 kB)\n","Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm_eval==0.4.8->-r requirements.txt (line 2))\n","  Downloading pathvalidate-3.3.1-py3-none-any.whl.metadata (12 kB)\n","Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm_eval==0.4.8->-r requirements.txt (line 2))\n","  Downloading tabledata-1.3.4-py3-none-any.whl.metadata (3.7 kB)\n","Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm_eval==0.4.8->-r requirements.txt (line 2))\n","  Downloading tcolorpy-0.1.7-py3-none-any.whl.metadata (6.3 kB)\n","Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.8->-r requirements.txt (line 2))\n","  Downloading typepy-1.3.4-py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval==0.4.8->-r requirements.txt (line 2)) (5.2.0)\n","Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n","\u001b[?25hDownloading lm_eval-0.4.8-py3-none-any.whl (3.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n","\u001b[?25hDownloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n","Downloading antlr4_python3_runtime-4.11.0-py3-none-any.whl (144 kB)\n","Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n","Downloading pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n","\u001b[?25hDownloading math_verify-0.8.0-py3-none-any.whl (29 kB)\n","Downloading latex2sympy2_extended-1.10.2-py3-none-any.whl (207 kB)\n","Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n","Downloading pybind11-3.0.1-py3-none-any.whl (293 kB)\n","Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n","Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n","Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n","Downloading pytablewriter-1.2.1-py3-none-any.whl (91 kB)\n","Downloading DataProperty-1.1.0-py3-none-any.whl (27 kB)\n","Downloading mbstrdecoder-1.1.4-py3-none-any.whl (7.9 kB)\n","Downloading pathvalidate-3.3.1-py3-none-any.whl (24 kB)\n","Downloading tabledata-1.3.4-py3-none-any.whl (11 kB)\n","Downloading tcolorpy-0.1.7-py3-none-any.whl (8.1 kB)\n","Downloading typepy-1.3.4-py3-none-any.whl (31 kB)\n","Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n","Building wheels for collected packages: rouge-score, sqlitedict, word2number\n","  Building wheel for rouge-score (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24987 sha256=cef017f7637dc0f5047f98ca7bc64e5d6ab2b5d7cf6e7c55fe48533eb2f45bf8\n","  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n","  Building wheel for sqlitedict (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16957 sha256=4be8c7f426d77d137daa096987f65d47b242335258f643e2499ddee679542916\n","  Stored in directory: /root/.cache/pip/wheels/7a/6f/21/fc016aef45ffcabe27129a2252f061387cbf278d2086225a64\n","  Building wheel for word2number (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5659 sha256=a66ac035559543a2262094c65996e65ec9f8e8ce3a30fab596ca9c148c2cfe7b\n","  Stored in directory: /root/.cache/pip/wheels/5b/79/fb/d25928e599c7e11fe4e00d32048cd74933f34a74c633d2aea6\n","Successfully built rouge-score sqlitedict word2number\n","Installing collected packages: word2number, sqlitedict, antlr4-python3-runtime, tcolorpy, pydantic-core, pybind11, portalocker, pathvalidate, mbstrdecoder, jsonlines, colorama, typepy, tqdm-multiprocess, sacrebleu, rouge-score, pydantic, latex2sympy2_extended, tokenizers, math_verify, transformers, DataProperty, accelerate, tabledata, evaluate, pytablewriter, lm_eval\n","\u001b[2K  Attempting uninstall: antlr4-python3-runtime\n","\u001b[2K    Found existing installation: antlr4-python3-runtime 4.9.3\n","\u001b[2K    Uninstalling antlr4-python3-runtime-4.9.3:\n","\u001b[2K      Successfully uninstalled antlr4-python3-runtime-4.9.3\n","\u001b[2K  Attempting uninstall: pydantic-core\n","\u001b[2K    Found existing installation: pydantic_core 2.33.2\n","\u001b[2K    Uninstalling pydantic_core-2.33.2:\n","\u001b[2K      Successfully uninstalled pydantic_core-2.33.2\n","\u001b[2K  Attempting uninstall: pydantic\n","\u001b[2K    Found existing installation: pydantic 2.11.10\n","\u001b[2K    Uninstalling pydantic-2.11.10:\n","\u001b[2K      Successfully uninstalled pydantic-2.11.10\n","\u001b[2K  Attempting uninstall: tokenizers\n","\u001b[2K    Found existing installation: tokenizers 0.22.1\n","\u001b[2K    Uninstalling tokenizers-0.22.1:\n","\u001b[2K      Successfully uninstalled tokenizers-0.22.1\n","\u001b[2K  Attempting uninstall: transformers\n","\u001b[2K    Found existing installation: transformers 4.57.1\n","\u001b[2K    Uninstalling transformers-4.57.1:\n","\u001b[2K      Successfully uninstalled transformers-4.57.1\n","\u001b[2K  Attempting uninstall: accelerate\n","\u001b[2K    Found existing installation: accelerate 1.11.0\n","\u001b[2K    Uninstalling accelerate-1.11.0:\n","\u001b[2K      Successfully uninstalled accelerate-1.11.0\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26/26\u001b[0m [lm_eval]\n","\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","omegaconf 2.3.0 requires antlr4-python3-runtime==4.9.*, but you have antlr4-python3-runtime 4.11.0 which is incompatible.\n","mcp 1.18.0 requires pydantic<3.0.0,>=2.11.0, but you have pydantic 2.10.6 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed DataProperty-1.1.0 accelerate-0.34.2 antlr4-python3-runtime-4.11.0 colorama-0.4.6 evaluate-0.4.6 jsonlines-4.0.0 latex2sympy2_extended-1.10.2 lm_eval-0.4.8 math_verify-0.8.0 mbstrdecoder-1.1.4 pathvalidate-3.3.1 portalocker-3.2.0 pybind11-3.0.1 pydantic-2.10.6 pydantic-core-2.27.2 pytablewriter-1.2.1 rouge-score-0.1.2 sacrebleu-2.5.1 sqlitedict-2.1.0 tabledata-1.3.4 tcolorpy-0.1.7 tokenizers-0.21.4 tqdm-multiprocess-0.0.11 transformers-4.49.0 typepy-1.3.4 word2number-1.1\n","Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n","Collecting datasets\n","  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (0.34.2)\n","Collecting accelerate\n","  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.35.3)\n","Collecting huggingface_hub\n","  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n","Collecting pyarrow>=21.0.0 (from datasets)\n","  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n","Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n","Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.1.10)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n","Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.6.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Downloading datasets-4.3.0-py3-none-any.whl (506 kB)\n","Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n","\u001b[?25hDownloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n","Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m143.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyarrow, huggingface_hub, datasets, accelerate\n","\u001b[2K  Attempting uninstall: pyarrow\n","\u001b[2K    Found existing installation: pyarrow 18.1.0\n","\u001b[2K    Uninstalling pyarrow-18.1.0:\n","\u001b[2K      Successfully uninstalled pyarrow-18.1.0\n","\u001b[2K  Attempting uninstall: huggingface_hub\n","\u001b[2K    Found existing installation: huggingface-hub 0.35.3\n","\u001b[2K    Uninstalling huggingface-hub-0.35.3:\n","\u001b[2K      Successfully uninstalled huggingface-hub-0.35.3\n","\u001b[2K  Attempting uninstall: datasets\n","\u001b[2K    Found existing installation: datasets 4.0.0\n","\u001b[2K    Uninstalling datasets-4.0.0:\n","\u001b[2K      Successfully uninstalled datasets-4.0.0\n","\u001b[2K  Attempting uninstall: accelerate\n","\u001b[2K    Found existing installation: accelerate 0.34.2\n","\u001b[2K    Uninstalling accelerate-0.34.2:\n","\u001b[2K      Successfully uninstalled accelerate-0.34.2\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [accelerate]\n","\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n","cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed accelerate-1.11.0 datasets-4.3.0 huggingface_hub-0.36.0 pyarrow-22.0.0\n"]}],"source":["!pip install -U pip\n","!pip install -r requirements.txt\n","!pip install -U datasets evaluate accelerate huggingface_hub"]},{"cell_type":"markdown","metadata":{"id":"EGRKY4sW0Z46"},"source":["## 4. （必要に応じて）Hugging Face にログイン\n","- private/gated モデルを Drive に保存済みであればスキップ可。\n","- Hub から直接取得する際は以下を実行し、トークンを入力してください。"],"id":"EGRKY4sW0Z46"},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17,"referenced_widgets":["5a8423d55ae74584aaa2c3d72a886e6a","47f071c97b8b49beb12e166b5946e1a8","b35d2fb58db843ea8d8a6b0025ed89eb","e9343e2387e449a18fa117af18533c46","aa2a3d51e53c479497240c5b56a28b79","62e29bcdad6041689c1ecbfd840ad43f","0bd88cfd42044c4aa66fe9284d5768b6","19aace28e0f74e7e8fc04d5bdb8b8c59","11385e4f703a4c03b54905a70bf37c37","3245ca36af3c48da9a0b1a1a418b52b9","240f7c4133af46229471c40d6877021a","c366a1d2b192452d91c96cea6e5b214f","f76bcce7c0e64176a6ea5a27743ebf1a","1f9e6d1cd1204ae98a4732ebc221ded5","fe1b35c3b5494c90ac77c373defefe90","b9c01f7a7224467aa13d418faf89fbe5","190c5234cb3242b8b93243f502f8cf5c","810c8d097b8241cfb717aa1903a88da2","8f1df22ecee64122afd67df9096ceba5","9d68b32d1f404308b274082faed42840"]},"id":"-eoMSFc50Z46","executionInfo":{"status":"ok","timestamp":1761537625369,"user_tz":-540,"elapsed":231,"user":{"displayName":"俊輔","userId":"03232639100731679356"}},"outputId":"688cd397-0c4a-4546-fc56-495515f14dc0"},"outputs":[{"output_type":"display_data","data":{"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a8423d55ae74584aaa2c3d72a886e6a"}},"metadata":{}}],"source":["from huggingface_hub import login\n","login()  # ★Hub からダウンロードする場合のみ実行"],"id":"-eoMSFc50Z46"},{"cell_type":"markdown","metadata":{"id":"hOj4Occh0Z46"},"source":["## 5. 評価設定を記入\n","- `MAIN_MODEL_PATH` / `DRAFT_MODEL_PATH` を Drive 内のダウンロード済みモデルに変更してください（HF Hub ID でも可）。\n","- サンプル数を絞りたい場合は `GSM8K_MAX_SAMPLES` / `HUMAN_MAX_SAMPLES` を小さめに設定します（`None` で全件）。\n","- PairSpec の受理ポリシーやドラフト深度もここで調整します。"],"id":"hOj4Occh0Z46"},{"cell_type":"code","execution_count":5,"id":"ea736fa1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ea736fa1","executionInfo":{"status":"ok","timestamp":1761537644659,"user_tz":-540,"elapsed":5,"user":{"displayName":"俊輔","userId":"03232639100731679356"}},"outputId":"8a0d7f47-5006-4e3d-ce36-410c61271945"},"outputs":[{"output_type":"stream","name":"stdout","text":["設定完了: main model = GSAI-ML/LLaDA-8B-Instruct\n"]}],"source":["from types import SimpleNamespace\n","\n","# === モデル/デバイス設定 ===\n","MAIN_MODEL_PATH = 'GSAI-ML/LLaDA-8B-Instruct'  # 例: ローカル格納済みモデル or HF Repo\n","DRAFT_MODEL_PATH = 'GSAI-ML/LLaDA-1.5'   # 例: ドラフト用ローカルモデル or HF Repo\n","VERIFY_DEVICE = 'cuda:0'\n","DRAFT_DEVICE = 'cuda:0'  # Colab では単一 GPU のため同一デバイスを想定\n","\n","# === 生成パラメータ ===\n","GEN_LENGTH = 128\n","BLOCK_SIZE = 32\n","BASELINE_STEPS = 128        # 通常 Fast-dLLM のステップ数\n","PARALLEL_THRESHOLD = 0.9    # 信頼度しきい値（必要に応じて調整）\n","\n","# === PairSpec 固有 ===\n","PAIRSPEC_ACCEPT_POLICY = 'lossless'  # 'lossless' or 'thresholded'\n","PAIRSPEC_ACCEPT_THRESHOLD = 2.0\n","PAIRSPEC_DRAFT_DEPTH = 2\n","PAIRSPEC_DRAFT_STEPS = None  # None -> 自動で steps/num_blocks\n","\n","# === 評価データ設定 ===\n","GSM8K_MAX_SAMPLES = 32        # None で全テストセット (1319)\n","HUMAN_MAX_SAMPLES = 32        # None で全 164 件\n","HUMAN_EVAL_TIMEOUT = 15       # コード実行のタイムアウト（秒）\n","HUMAN_EVAL_MAX_WORKERS = 4    # 同時テスト実行数\n","\n","BASELINE_ARGS = SimpleNamespace(\n","    gen_length=GEN_LENGTH,\n","    steps=BASELINE_STEPS,\n","    block_size=BLOCK_SIZE,\n","    use_cache=True,\n","    if_cache_position=True,\n","    threshold=PARALLEL_THRESHOLD,\n",")\n","\n","PAIRSPEC_ARGS = SimpleNamespace(\n","    gen_length=GEN_LENGTH,\n","    steps=BASELINE_STEPS,\n","    block_size=BLOCK_SIZE,\n","    use_cache=True,\n","    if_cache_position=True,\n","    threshold=PARALLEL_THRESHOLD,\n","    draft_model=DRAFT_MODEL_PATH,\n","    draft_device=DRAFT_DEVICE,\n","    draft_depth=PAIRSPEC_DRAFT_DEPTH,\n","    draft_steps=PAIRSPEC_DRAFT_STEPS,\n","    accept_policy=PAIRSPEC_ACCEPT_POLICY,\n","    accept_threshold=PAIRSPEC_ACCEPT_THRESHOLD,\n",")\n","\n","print('設定完了: main model =', MAIN_MODEL_PATH)"]},{"cell_type":"markdown","id":"f889d894","metadata":{"id":"f889d894"},"source":["## 5.1 モデルパスの解決（ローカル優先）\n","- 指定ディレクトリが存在しない場合は Hugging Face から Drive に自動ダウンロードします。"]},{"cell_type":"code","execution_count":6,"id":"1187e3f3","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1187e3f3","executionInfo":{"status":"ok","timestamp":1761537646529,"user_tz":-540,"elapsed":8,"user":{"displayName":"俊輔","userId":"03232639100731679356"}},"outputId":"3a13cf62-b186-47a5-d4dc-35f211b75e49"},"outputs":[{"output_type":"stream","name":"stdout","text":["既存のキャッシュを使用: /content/drive/MyDrive/hf_models/GSAI-ML__LLaDA-8B-Instruct\n","既存のキャッシュを使用: /content/drive/MyDrive/hf_models/GSAI-ML__LLaDA-1.5\n","Main model path: /content/drive/MyDrive/hf_models/GSAI-ML__LLaDA-8B-Instruct\n","Draft model path: /content/drive/MyDrive/hf_models/GSAI-ML__LLaDA-1.5\n"]}],"source":["from huggingface_hub import snapshot_download\n","\n","LOCAL_MODEL_CACHE_ROOT = '/content/drive/MyDrive/hf_models'\n","\n","def ensure_local_model(identifier: str, cache_root: str = LOCAL_MODEL_CACHE_ROOT):\n","    if identifier is None or identifier == '':\n","        return None\n","    if os.path.isdir(identifier):\n","        print(f\"ローカルパスを使用: {identifier}\")\n","        return identifier\n","    safe_name = identifier.replace('/', '__')\n","    target_dir = os.path.join(cache_root, safe_name)\n","    if os.path.isdir(target_dir):\n","        print(f\"既存のキャッシュを使用: {target_dir}\")\n","        return target_dir\n","    os.makedirs(target_dir, exist_ok=True)\n","    print(f\"Hugging Face から {identifier} をダウンロード -> {target_dir}\")\n","    snapshot_download(repo_id=identifier, local_dir=target_dir, local_dir_use_symlinks=False)\n","    return target_dir\n","\n","MAIN_MODEL_PATH_RESOLVED = ensure_local_model(MAIN_MODEL_PATH)\n","DRAFT_MODEL_PATH_RESOLVED = ensure_local_model(DRAFT_MODEL_PATH)\n","PAIRSPEC_ARGS.draft_model = DRAFT_MODEL_PATH_RESOLVED\n","print('Main model path:', MAIN_MODEL_PATH_RESOLVED)\n","print('Draft model path:', DRAFT_MODEL_PATH_RESOLVED)"]},{"cell_type":"markdown","id":"79a15f81","metadata":{"id":"79a15f81"},"source":["## 6. モデルとトークナイザの読み込み\n","- Drive 上にあるモデルフォルダを `from_pretrained` で直接指定します。\n","- bfloat16 / eval モードで読み込みます。"]},{"cell_type":"code","execution_count":7,"id":"5ec70e77","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":104,"referenced_widgets":["f52048395b7f49f48f1ac19fafc16b34","148e3769d43f4483baf3aa60fe03604c","53a88814dc2349acaea8444c887f988c","1f708194e72a4fda8b020b31d7b42cd7","aa6a2628500543b4b1b032230f330618","5b6b797e517e4e79a55cd67fc4f4fba9","7cf1504833a844ceaeca27b6c6eda1ff","cc0665b7807547799b2a29abba6d52cf","3a4bcd2c52144a2fb14ecb508a2de1bf","7845b51e3ff74d6ebc7f2d3cfe09ef30","b1599a90674c491aa30255ad6a111fca"]},"id":"5ec70e77","executionInfo":{"status":"ok","timestamp":1761537855243,"user_tz":-540,"elapsed":207097,"user":{"displayName":"俊輔","userId":"03232639100731679356"}},"outputId":"e957d53c-ffd6-40d2-fbef-0686ba558f6d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Warning: 要求された VERIFY_DEVICE=cuda:0 と実際の device=cuda が異なります。\n"]},{"output_type":"stream","name":"stderr","text":["The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f52048395b7f49f48f1ac19fafc16b34"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["モデル / トークナイザ読み込み完了\n"]}],"source":["import torch\n","from transformers import AutoTokenizer\n","from llada.model.modeling_llada import LLaDAModelLM\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","if str(device) != VERIFY_DEVICE:\n","    print(f\"Warning: 要求された VERIFY_DEVICE={VERIFY_DEVICE} と実際の device={device} が異なります。\")\n","\n","MODEL_LOAD_PATH = MAIN_MODEL_PATH_RESOLVED or MAIN_MODEL_PATH\n","if MODEL_LOAD_PATH is None:\n","    raise ValueError(\"MAIN_MODEL_PATH を設定してください。\")\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_LOAD_PATH, trust_remote_code=True)\n","model = LLaDAModelLM.from_pretrained(\n","    MODEL_LOAD_PATH,\n","    trust_remote_code=True,\n","    torch_dtype=torch.bfloat16,\n",").to(device).eval()\n","\n","print('モデル / トークナイザ読み込み完了')"]},{"cell_type":"markdown","metadata":{"id":"HcLKD5ZV0Z47"},"source":["## 7. ヘルパー関数の定義\n","- プロンプト生成、最終数値抽出、PairSpec 生成クラス、評価ループなどを実装します。\n","- PairSpec 生成は `specdraft` モジュールを直接利用し、ドラフトワーカを使い回せるようクラス化しています。"],"id":"HcLKD5ZV0Z47"},{"cell_type":"code","execution_count":8,"id":"9f763b25","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9f763b25","executionInfo":{"status":"ok","timestamp":1761537861136,"user_tz":-540,"elapsed":5891,"user":{"displayName":"俊輔","userId":"03232639100731679356"}},"outputId":"9abb3fd5-e89c-46e2-eb07-f226232d9e96"},"outputs":[{"output_type":"stream","name":"stdout","text":["ヘルパー関数を定義しました。\n"]}],"source":["import math\n","import re\n","import time\n","import json\n","import pandas as pd\n","from datasets import load_dataset\n","from tqdm import tqdm\n","from llada.chat import _select_generator_name, _select_generator_fn\n","from specdraft.dispatcher import DraftRequest, start_draft_worker, shutdown_draft_worker\n","from specdraft.acceptor import compute_prefix_hash, verify_and_commit\n","from specdraft.kv_manager import KVManager\n","\n","\n","\n","\n","\n","def strip_code_fence(text: str) -> str:\n","    stripped = text.strip()\n","    if stripped.startswith(\"```\"):\n","        lines = stripped.splitlines()\n","        if lines and lines[0].startswith(\"```\"):\n","            lines = lines[1:]\n","        while lines and lines[-1].strip() == \"```\":\n","            lines = lines[:-1]\n","        stripped = \"\\n\".join(lines)\n","    return stripped.strip()\n","\n","\n","\n","MASK_TOKEN_ID = tokenizer.mask_token_id or 126336\n","\n","def apply_chat_template(user_text: str) -> str:\n","    messages = [\n","        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","        {\"role\": \"user\", \"content\": user_text.strip()},\n","    ]\n","    return tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n","\n","def build_gsm8k_prompt(question: str) -> str:\n","    instruction = (\n","        \"Solve the math word problem step by step and output the final numeric answer in the format '#### value'.\\n\\n\"\n","        f\"Problem: {question.strip()}\\n\\nAnswer:\"\n","    )\n","    return apply_chat_template(instruction)\n","\n","def build_humaneval_prompt(sample: dict) -> str:\n","    instruction = (\n","        \"Complete the Python function specification below. Return only executable Python code.\\n\\n\"\n","        f\"{sample['prompt'].strip()}\"\n","    )\n","    return apply_chat_template(instruction)\n","\n","def extract_gsm8k_answer(text: str):\n","    matches = re.findall(r\"####\\s*([-+]?\\d+(?:\\.\\d+)?)\", text)\n","    if matches:\n","        try:\n","            return float(matches[-1])\n","        except ValueError:\n","            return None\n","    return None\n","\n","def count_new_tokens(tokens: torch.Tensor) -> int:\n","    if tokens is None or tokens.numel() == 0:\n","        return 0\n","    return tokens.shape[1]\n","\n","def run_baseline_generation(prompt_ids: torch.Tensor, args) -> tuple:\n","    generator_name = _select_generator_name(args)\n","    generator_fn = _select_generator_fn(generator_name)\n","    out, nfe = generator_fn(\n","        model,\n","        prompt_ids,\n","        steps=args.steps,\n","        gen_length=args.gen_length,\n","        block_length=args.block_size,\n","        temperature=0.0,\n","        remasking='low_confidence',\n","        threshold=args.threshold,\n","    )\n","    new_tokens = out[:, prompt_ids.shape[1]:]\n","    stats = {\n","        'accepted_blocks': 0,\n","        'attempted_blocks': args.gen_length // args.block_size,\n","        'draft_nfe': 0,\n","        'verify_nfe': 0,\n","        'fallback_nfe': nfe,\n","        'total_nfe': nfe,\n","    }\n","    return new_tokens, nfe, stats\n","\n","def run_block_generation(current_prompt: torch.Tensor, args, steps_per_block: int, generator_name: str):\n","    generator_fn = _select_generator_fn(generator_name)\n","    out, nfe = generator_fn(\n","        model,\n","        current_prompt,\n","        steps=steps_per_block,\n","        gen_length=args.block_size,\n","        block_length=args.block_size,\n","        temperature=0.0,\n","        remasking='low_confidence',\n","        threshold=args.threshold,\n","    )\n","    block_tensor = out[:, -args.block_size:]\n","    return block_tensor, int(nfe)\n","\n","class PairSpecSession:\n","    def __init__(self, args, tokenizer):\n","        self.args = args\n","        self.tokenizer = tokenizer\n","        self.generator_name = _select_generator_name(args)\n","        self.worker, self.request_queue, self.spec_queue = start_draft_worker(\n","            backend='llada',\n","            model_path=args.draft_model,\n","            device=args.draft_device,\n","            generator=self.generator_name,\n","            max_depth=args.draft_depth,\n","            dtype=torch.bfloat16,\n","        )\n","\n","    def close(self):\n","        shutdown_draft_worker(self.worker, self.request_queue)\n","        if self.spec_queue:\n","            self.spec_queue.close()\n","\n","    def generate(self, prompt_ids: torch.Tensor):\n","        args = self.args\n","        block_size = args.block_size\n","        gen_length = args.gen_length\n","        if gen_length % block_size != 0:\n","            raise ValueError('gen_length は block_size で割り切れる必要があります')\n","        total_blocks = gen_length // block_size\n","        steps_per_block = max(1, args.steps // max(total_blocks, 1))\n","        kv_mgr = KVManager(model)\n","        prefix_hash = compute_prefix_hash(prompt_ids[0].tolist())\n","        stats = {\n","            'accepted_blocks': 0,\n","            'attempted_blocks': total_blocks,\n","            'draft_nfe': 0,\n","            'verify_nfe': 0,\n","            'fallback_nfe': 0,\n","        }\n","        pending_blocks = set()\n","\n","        def enqueue(block_id: int, current_prompt: torch.Tensor, current_hash: str):\n","            if block_id >= total_blocks or block_id in pending_blocks:\n","                return\n","            req = DraftRequest(\n","                block_id=block_id,\n","                prefix_tokens=current_prompt[0].tolist(),\n","                prefix_hash=current_hash,\n","                block_size=block_size,\n","                steps_per_block=max(1, args.draft_steps or steps_per_block),\n","                temperature=0.0,\n","                remasking='low_confidence',\n","                threshold=args.threshold,\n","                generator=self.generator_name,\n","            )\n","            self.request_queue.put(req)\n","            pending_blocks.add(block_id)\n","\n","        enqueue(0, prompt_ids, prefix_hash)\n","        current_prompt = prompt_ids.clone()\n","        generated_segments = []\n","\n","        for block_id in range(total_blocks):\n","            draft = self.spec_queue.try_get(block_id)\n","            if draft is not None:\n","                pending_blocks.discard(block_id)\n","                stats['draft_nfe'] += draft.nfe\n","                if draft.prefix_hash != prefix_hash:\n","                    draft = None\n","\n","            accepted_tensor = None\n","            if draft is not None:\n","                verification = verify_and_commit(\n","                    draft,\n","                    model,\n","                    current_prompt,\n","                    mask_token_id=MASK_TOKEN_ID,\n","                    policy=args.accept_policy,\n","                    threshold=args.accept_threshold,\n","                )\n","                stats['verify_nfe'] += verification.nfe\n","                if verification.accepted:\n","                    stats['accepted_blocks'] += 1\n","                    accepted_tensor = torch.tensor(\n","                        verification.accepted_tokens,\n","                        dtype=current_prompt.dtype,\n","                        device=current_prompt.device,\n","                    ).unsqueeze(0)\n","\n","            if accepted_tensor is None:\n","                block_tensor, nfe_block = run_block_generation(\n","                    current_prompt,\n","                    args,\n","                    steps_per_block,\n","                    self.generator_name,\n","                )\n","                stats['fallback_nfe'] += nfe_block\n","            else:\n","                block_tensor = accepted_tensor\n","\n","            current_prompt = torch.cat([current_prompt, block_tensor], dim=1)\n","            prefix_hash = compute_prefix_hash(current_prompt[0].tolist())\n","            kv_mgr.recompute_on_commit(prefix_hash)\n","            generated_segments.append(block_tensor)\n","            enqueue(block_id + 1, current_prompt, prefix_hash)\n","\n","        full_generation = torch.cat(generated_segments, dim=1) if generated_segments else torch.empty((1, 0), dtype=current_prompt.dtype, device=current_prompt.device)\n","        stats['total_nfe'] = stats['verify_nfe'] + stats['fallback_nfe']\n","        return full_generation, stats['total_nfe'], stats\n","\n","def generate_text(prompt_text: str, mode: str, pair_session: PairSpecSession = None):\n","    encoded = tokenizer(prompt_text, return_tensors='pt').input_ids.to(device)\n","    start = time.perf_counter()\n","    if mode == 'pairspec':\n","        if pair_session is None:\n","            raise ValueError('PairSpec モードには PairSpecSession が必要です')\n","        new_tokens, nfe, stats = pair_session.generate(encoded)\n","    else:\n","        new_tokens, nfe, stats = run_baseline_generation(encoded, BASELINE_ARGS)\n","    elapsed = time.perf_counter() - start\n","    decoded = tokenizer.batch_decode(new_tokens, skip_special_tokens=True)[0].strip()\n","    token_count = count_new_tokens(new_tokens)\n","    tps = token_count / elapsed if elapsed > 0 else float('inf')\n","    metrics = {\n","        'latency': elapsed,\n","        'tokens': token_count,\n","        'tps': tps,\n","        'nfe': nfe,\n","        **stats,\n","    }\n","    return decoded, metrics\n","\n","print('ヘルパー関数を定義しました。')"]},{"cell_type":"markdown","id":"929aa3e9","metadata":{"id":"929aa3e9"},"source":["## 8. 評価ルーチン\n","- GSM8K: 最終数値一致で正解判定。\n","- HumanEval: Hugging Face `code_eval` を用いて pass@1 を算出。\n","- 速度指標はサンプルごとのログから集計します。"]},{"cell_type":"code","execution_count":20,"id":"e662e154","metadata":{"id":"e662e154","executionInfo":{"status":"ok","timestamp":1761538828128,"user_tz":-540,"elapsed":9,"user":{"displayName":"俊輔","userId":"03232639100731679356"}}},"outputs":[],"source":["import evaluate\n","\n","def evaluate_gsm8k(mode: str, max_samples: int = None, pair_session: PairSpecSession = None):\n","    dataset = load_dataset('openai/gsm8k', 'main', split='test')\n","    if max_samples is not None:\n","        dataset = dataset.select(range(min(max_samples, len(dataset))))\n","    records = []\n","    for idx, sample in enumerate(tqdm(dataset, desc=f'GSM8K [{mode}]')):\n","        prompt = build_gsm8k_prompt(sample['question'])\n","        completion, metrics = generate_text(prompt, mode, pair_session)\n","        pred = extract_gsm8k_answer(completion)\n","        gold = extract_gsm8k_answer(sample['answer'])\n","        correct = int(pred is not None and gold is not None and math.isclose(pred, gold))\n","        records.append({\n","            'dataset': 'gsm8k',\n","            'mode': mode,\n","            'index': idx,\n","            'question': sample['question'],\n","            'gold_answer': gold,\n","            'prediction_text': completion,\n","            'prediction_value': pred,\n","            'correct': correct,\n","            **metrics,\n","        })\n","    df = pd.DataFrame(records)\n","    summary = {\n","        'dataset': 'gsm8k',\n","        'mode': mode,\n","        'samples': len(df),\n","        'accuracy': df['correct'].mean() if len(df) else float('nan'),\n","        'avg_latency': df['latency'].mean() if len(df) else float('inf'),\n","        'avg_tps': df['tps'].mean() if len(df) else float('inf'),\n","        'avg_nfe': df['nfe'].mean() if len(df) else float('nan'),\n","        'accepted_block_ratio': (df['accepted_blocks'] / df['attempted_blocks']).mean() if 'accepted_blocks' in df else 0.0,\n","    }\n","    return df, summary\n","\n","def evaluate_humaneval(mode: str, max_samples: int = None, pair_session: PairSpecSession = None):\n","    dataset = load_dataset('openai/openai_humaneval', split='test')\n","    if max_samples is not None:\n","        dataset = dataset.select(range(min(max_samples, len(dataset))))\n","\n","    records = []\n","    predictions = []      # list[list[str]]\n","    tests = []            # list[str]\n","\n","    for idx, sample in enumerate(tqdm(dataset, desc=f'HumanEval [{mode}]')):\n","        prompt = build_humaneval_prompt(sample)\n","        completion, metrics = generate_text(prompt, mode, pair_session)\n","        clean_completion = strip_code_fence(completion)\n","\n","        # ここが重要: 文字列ではなく「リストで包む」\n","        predictions.append([clean_completion])\n","        tests.append(sample['test'])\n","\n","        records.append({\n","            'dataset': 'humaneval',\n","            'mode': mode,\n","            'task_id': sample['task_id'],\n","            'prompt': sample['prompt'],\n","            'completion': clean_completion,\n","            **metrics,\n","        })\n","\n","    # 事前チェック（実行時に早期に型不一致を検知）\n","    assert isinstance(predictions, list) and all(isinstance(cands, list) and all(isinstance(s, str) for s in cands) for cands in predictions)\n","    assert isinstance(tests, list) and all(isinstance(t, str) for t in tests)\n","\n","    import evaluate\n","    code_eval = evaluate.load('code_eval')\n","\n","    # code_eval は (pass_at_k, results) を返す\n","    pass_at_k, results = code_eval.compute(\n","        references=tests,          # list[str]\n","        predictions=predictions,   # list[list[str]]\n","        k=[1],\n","        timeout=HUMAN_EVAL_TIMEOUT,\n","    )\n","\n","    df = pd.DataFrame(records)\n","    summary = {\n","        'dataset': 'humaneval',\n","        'mode': mode,\n","        'samples': len(df),\n","        'pass@1': pass_at_k.get('pass@1', float('nan')),\n","        'avg_latency': df['latency'].mean() if len(df) else float('nan'),\n","        'avg_tps': df['tps'].mean() if len(df) else float('inf'),\n","        'avg_nfe': df['nfe'].mean() if len(df) else float('nan'),\n","        'accepted_block_ratio': (df['accepted_blocks'] / df['attempted_blocks']).mean() if 'accepted_blocks' in df else 0.0,\n","    }\n","    eval_result = {'pass_at_k': pass_at_k, 'results': results}\n","    return df, summary, eval_result\n"]},{"cell_type":"markdown","metadata":{"id":"Su2BEwXq0Z48"},"source":["## 9. ベースライン & PairSpec 評価を実行\n","- それぞれのデータセットについて、PairSpec セッションを初期化してから計測します。\n","- 実行には時間がかかるので、`MAX_SAMPLES` を調整しながら進めてください。"],"id":"Su2BEwXq0Z48"},{"cell_type":"code","source":["import os\n","os.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\""],"metadata":{"id":"Ib2uNRl-4pgs","executionInfo":{"status":"ok","timestamp":1761538830162,"user_tz":-540,"elapsed":2,"user":{"displayName":"俊輔","userId":"03232639100731679356"}}},"id":"Ib2uNRl-4pgs","execution_count":21,"outputs":[]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":687},"id":"NlZvI0G70Z48","executionInfo":{"status":"error","timestamp":1761539110500,"user_tz":-540,"elapsed":279448,"user":{"displayName":"俊輔","userId":"03232639100731679356"}},"outputId":"c4137761-373d-4ab9-ae7c-67127cef1d6b"},"outputs":[{"output_type":"stream","name":"stderr","text":["GSM8K [baseline]: 100%|██████████| 32/32 [00:44<00:00,  1.40s/it]\n","HumanEval [baseline]: 100%|██████████| 32/32 [00:51<00:00,  1.61s/it]\n","Process DraftWorker-65:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n","    self.run()\n","  File \"/content/drive/MyDrive/PairSpec-dLLM/specdraft/dispatcher.py\", line 264, in run\n","    self._ensure_model()\n","  File \"/content/drive/MyDrive/PairSpec-dLLM/specdraft/dispatcher.py\", line 204, in _ensure_model\n","    torch.cuda.set_device(self.device)\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py\", line 569, in set_device\n","    torch._C._cuda_setDevice(device)\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py\", line 398, in _lazy_init\n","    raise RuntimeError(\n","RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n","GSM8K [pairspec]:   3%|▎         | 1/32 [02:57<1:31:57, 177.99s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-664041879.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mpairspec_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPairSpecSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPAIRSPEC_ARGS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mgsm8k_pairspec_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgsm8k_pairspec_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_gsm8k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pairspec'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGSM8K_MAX_SAMPLES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairspec_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mall_dfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgsm8k_pairspec_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0msummaries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgsm8k_pairspec_summary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-537182365.py\u001b[0m in \u001b[0;36mevaluate_gsm8k\u001b[0;34m(mode, max_samples, pair_session)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'GSM8K [{mode}]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_gsm8k_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mcompletion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_gsm8k_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompletion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mgold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_gsm8k_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3023338677.py\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(prompt_text, mode, pair_session)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpair_session\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PairSpec モードには PairSpecSession が必要です'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mnew_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpair_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mnew_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_baseline_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBASELINE_ARGS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3023338677.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompt_ids)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mpending_blocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0menqueue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix_hash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0mcurrent_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mgenerated_segments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3023338677.py\u001b[0m in \u001b[0;36menqueue\u001b[0;34m(block_id, current_prompt, current_hash)\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             )\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0mpending_blocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/multiprocessing/queues.py\u001b[0m in \u001b[0;36mput\u001b[0;34m(self, obj, block, timeout)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Queue {self!r} is closed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mFull\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["all_dfs = []\n","summaries = []\n","\n","# --- ベースライン ---\n","gsm8k_baseline_df, gsm8k_baseline_summary = evaluate_gsm8k('baseline', GSM8K_MAX_SAMPLES)\n","all_dfs.append(gsm8k_baseline_df)\n","summaries.append(gsm8k_baseline_summary)\n","\n","humaneval_baseline_df, humaneval_baseline_summary, humaneval_baseline_eval = evaluate_humaneval('baseline', HUMAN_MAX_SAMPLES)\n","all_dfs.append(humaneval_baseline_df)\n","summaries.append(humaneval_baseline_summary)\n","\n","# --- PairSpec ---\n","pairspec_session = PairSpecSession(PAIRSPEC_ARGS, tokenizer)\n","try:\n","    gsm8k_pairspec_df, gsm8k_pairspec_summary = evaluate_gsm8k('pairspec', GSM8K_MAX_SAMPLES, pairspec_session)\n","    all_dfs.append(gsm8k_pairspec_df)\n","    summaries.append(gsm8k_pairspec_summary)\n","\n","    humaneval_pairspec_df, humaneval_pairspec_summary, humaneval_pairspec_eval = evaluate_humaneval('pairspec', HUMAN_MAX_SAMPLES, pairspec_session)\n","    all_dfs.append(humaneval_pairspec_df)\n","    summaries.append(humaneval_pairspec_summary)\n","finally:\n","    pairspec_session.close()\n","\n","full_results_df = pd.concat(all_dfs, ignore_index=True)\n","summary_df = pd.DataFrame(summaries)\n","print('評価完了')"],"id":"NlZvI0G70Z48"},{"cell_type":"markdown","metadata":{"id":"SfcZfDfa0Z48"},"source":["## 10. 指標サマリと詳細ログ\n","- `summary_df` でデータセット毎の主要指標を確認できます。\n","- `full_results_df` をフィルタすればサンプルごとのログ（正誤・速度・PairSpec 受理率など）を参照できます。\n","- 必要に応じて CSV で Drive に保存してください。"],"id":"SfcZfDfa0Z48"},{"cell_type":"code","execution_count":null,"metadata":{"id":"86jx7yWv0Z48"},"outputs":[],"source":["summary_df"],"id":"86jx7yWv0Z48"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ekG69S2s0Z48"},"outputs":[],"source":["# 例: 先頭5件だけ表示\n","full_results_df.head()"],"id":"ekG69S2s0Z48"},{"cell_type":"code","execution_count":null,"metadata":{"id":"iw7DZihN0Z48"},"outputs":[],"source":["# 任意: CSV として Drive に保存\n","# summary_df.to_csv('/content/drive/MyDrive/pairspec_eval_summary.csv', index=False)\n","# full_results_df.to_csv('/content/drive/MyDrive/pairspec_eval_details.csv', index=False)\n","print('必要に応じて CSV 保存コマンドを有効化してください。')"],"id":"iw7DZihN0Z48"},{"cell_type":"markdown","metadata":{"id":"sgzuGJvm0Z48"},"source":["## 11. 追加メモ\n","- HumanEval の `code_eval` は Python コードを実行するため、信頼できる環境でのみ実行してください。\n","- PairSpec のドラフト/検証を単一 GPU で同時に走らせるとメモリ使用量が増えるため、必要に応じて `GEN_LENGTH` や `BLOCK_SIZE`、`draft_depth` を調整してください。\n","- `PAIRSPEC_ARGS` の `accept_policy='thresholded'` に切り替えると、ドラフト受理を信頼度しきい値ベースに変更できます。"],"id":"sgzuGJvm0Z48"}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":""},"colab":{"provenance":[],"gpuType":"A100"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"5a8423d55ae74584aaa2c3d72a886e6a":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":[],"layout":"IPY_MODEL_0bd88cfd42044c4aa66fe9284d5768b6"}},"47f071c97b8b49beb12e166b5946e1a8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19aace28e0f74e7e8fc04d5bdb8b8c59","placeholder":"​","style":"IPY_MODEL_11385e4f703a4c03b54905a70bf37c37","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"b35d2fb58db843ea8d8a6b0025ed89eb":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_3245ca36af3c48da9a0b1a1a418b52b9","placeholder":"​","style":"IPY_MODEL_240f7c4133af46229471c40d6877021a","value":""}},"e9343e2387e449a18fa117af18533c46":{"model_module":"@jupyter-widgets/controls","model_name":"CheckboxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_c366a1d2b192452d91c96cea6e5b214f","style":"IPY_MODEL_f76bcce7c0e64176a6ea5a27743ebf1a","value":true}},"aa2a3d51e53c479497240c5b56a28b79":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_1f9e6d1cd1204ae98a4732ebc221ded5","style":"IPY_MODEL_fe1b35c3b5494c90ac77c373defefe90","tooltip":""}},"62e29bcdad6041689c1ecbfd840ad43f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9c01f7a7224467aa13d418faf89fbe5","placeholder":"​","style":"IPY_MODEL_190c5234cb3242b8b93243f502f8cf5c","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"0bd88cfd42044c4aa66fe9284d5768b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"19aace28e0f74e7e8fc04d5bdb8b8c59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11385e4f703a4c03b54905a70bf37c37":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3245ca36af3c48da9a0b1a1a418b52b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"240f7c4133af46229471c40d6877021a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c366a1d2b192452d91c96cea6e5b214f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f76bcce7c0e64176a6ea5a27743ebf1a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1f9e6d1cd1204ae98a4732ebc221ded5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe1b35c3b5494c90ac77c373defefe90":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"b9c01f7a7224467aa13d418faf89fbe5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"190c5234cb3242b8b93243f502f8cf5c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"810c8d097b8241cfb717aa1903a88da2":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f1df22ecee64122afd67df9096ceba5","placeholder":"​","style":"IPY_MODEL_9d68b32d1f404308b274082faed42840","value":"Connecting..."}},"8f1df22ecee64122afd67df9096ceba5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d68b32d1f404308b274082faed42840":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f52048395b7f49f48f1ac19fafc16b34":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_148e3769d43f4483baf3aa60fe03604c","IPY_MODEL_53a88814dc2349acaea8444c887f988c","IPY_MODEL_1f708194e72a4fda8b020b31d7b42cd7"],"layout":"IPY_MODEL_aa6a2628500543b4b1b032230f330618"}},"148e3769d43f4483baf3aa60fe03604c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b6b797e517e4e79a55cd67fc4f4fba9","placeholder":"​","style":"IPY_MODEL_7cf1504833a844ceaeca27b6c6eda1ff","value":"Loading checkpoint shards: 100%"}},"53a88814dc2349acaea8444c887f988c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc0665b7807547799b2a29abba6d52cf","max":6,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3a4bcd2c52144a2fb14ecb508a2de1bf","value":6}},"1f708194e72a4fda8b020b31d7b42cd7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7845b51e3ff74d6ebc7f2d3cfe09ef30","placeholder":"​","style":"IPY_MODEL_b1599a90674c491aa30255ad6a111fca","value":" 6/6 [02:14&lt;00:00, 21.03s/it]"}},"aa6a2628500543b4b1b032230f330618":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b6b797e517e4e79a55cd67fc4f4fba9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7cf1504833a844ceaeca27b6c6eda1ff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cc0665b7807547799b2a29abba6d52cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a4bcd2c52144a2fb14ecb508a2de1bf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7845b51e3ff74d6ebc7f2d3cfe09ef30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1599a90674c491aa30255ad6a111fca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}