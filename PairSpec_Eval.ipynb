{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PairSpec-dLLM 評価ノートブック\n",
    "\n",
    "Google Drive 上に配置済みの Fast-dLLM/PairSpec-dLLM を直接参照し、\n",
    "PairSpec（先行ドラフト併走）あり/なしの生成性能を **GSM8K (openai/gsm8k)** と **HumanEval (openai/openai_humaneval)** で比較します。\n",
    "\n",
    "- 速度系指標: latency / tokens per second / forward passes (NFE)\n",
    "- 精度系指標: GSM8K Accuracy, HumanEval Pass@1\n",
    "- モデルは Drive 上にダウンロード済みのものを直接使用（ローカル複製なし）\n",
    "- PairSpec 有効時は draft モデルを別 GPU/同 GPU 上で並列起動します"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 手順概要\n",
    "1. Google Drive をマウントし、既存の `PairSpec-dLLM` ルートへ `os.chdir`。\n",
    "2. 依存関係をインストールし、（必要なら）Hugging Face でログイン。\n",
    "3. 評価用の共通設定（モデルパス・ブロック長・サンプル数など）を記入。\n",
    "4. ノートブック内の関数でベースライン / PairSpec の両方を実行。\n",
    "5. 指標サマリとサンプルごとの詳細ログを確認。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Google Drive をマウント（※既にマウント済みならスキップ可）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 作業ディレクトリと環境変数のセット\n",
    "- `DRIVE_PROJECT_DIR` を Drive 上の `PairSpec-dLLM` ルートに変更してください。\n",
    "- 既存ファイルをそのまま参照するため、ローカルへの rsync/コピーは行いません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "DRIVE_PROJECT_DIR = '/content/drive/MyDrive/PairSpec-dLLM'  # ★必要に応じて変更\n",
    "\n",
    "if not os.path.isdir(DRIVE_PROJECT_DIR):\n",
    "    raise FileNotFoundError(f'{DRIVE_PROJECT_DIR} が見つかりません。パスを確認してください。')\n",
    "\n",
    "os.chdir(DRIVE_PROJECT_DIR)\n",
    "if DRIVE_PROJECT_DIR not in sys.path:\n",
    "    sys.path.insert(0, DRIVE_PROJECT_DIR)\n",
    "\n",
    "os.environ['HF_ALLOW_CODE_EVAL'] = '1'\n",
    "os.environ['HF_DATASETS_TRUST_REMOTE_CODE'] = '1'\n",
    "print('Working directory:', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 依存パッケージのインストール\n",
    "- プロジェクト付属の `requirements.txt` に加え、評価で用いる `datasets` / `evaluate` / `accelerate` を最新化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ab7288",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pip\n",
    "!pip install -r requirements.txt\n",
    "!pip install -U datasets evaluate accelerate huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. （必要に応じて）Hugging Face にログイン\n",
    "- private/gated モデルを Drive に保存済みであればスキップ可。\n",
    "- Hub から直接取得する際は以下を実行し、トークンを入力してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()  # ★Hub からダウンロードする場合のみ実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 評価設定を記入\n",
    "- `MAIN_MODEL_PATH` / `DRAFT_MODEL_PATH` を Drive 内のダウンロード済みモデルに変更してください（HF Hub ID でも可）。\n",
    "- サンプル数を絞りたい場合は `GSM8K_MAX_SAMPLES` / `HUMAN_MAX_SAMPLES` を小さめに設定します（`None` で全件）。\n",
    "- PairSpec の受理ポリシーやドラフト深度もここで調整します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea736fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "# === モデル/デバイス設定 ===\n",
    "MAIN_MODEL_PATH = 'GSAI-ML/LLaDA-8B-Instruct'  # 例: ローカル格納済みモデル or HF Repo\n",
    "DRAFT_MODEL_PATH = 'GSAI-ML/LLaDA-1.5'   # 例: ドラフト用ローカルモデル or HF Repo\n",
    "VERIFY_DEVICE = 'cuda:0'\n",
    "DRAFT_DEVICE = 'cuda:0'  # Colab では単一 GPU のため同一デバイスを想定\n",
    "\n",
    "# === 生成パラメータ ===\n",
    "GEN_LENGTH = 128\n",
    "BLOCK_SIZE = 32\n",
    "BASELINE_STEPS = 128        # 通常 Fast-dLLM のステップ数\n",
    "PARALLEL_THRESHOLD = 0.9    # 信頼度しきい値（必要に応じて調整）\n",
    "\n",
    "# === PairSpec 固有 ===\n",
    "PAIRSPEC_ACCEPT_POLICY = 'lossless'  # 'lossless' or 'thresholded'\n",
    "PAIRSPEC_ACCEPT_THRESHOLD = 2.0\n",
    "PAIRSPEC_DRAFT_DEPTH = 2\n",
    "PAIRSPEC_DRAFT_STEPS = None  # None -> 自動で steps/num_blocks\n",
    "\n",
    "# === 評価データ設定 ===\n",
    "GSM8K_MAX_SAMPLES = 32        # None で全テストセット (1319)\n",
    "HUMAN_MAX_SAMPLES = 32        # None で全 164 件\n",
    "HUMAN_EVAL_TIMEOUT = 15       # コード実行のタイムアウト（秒）\n",
    "HUMAN_EVAL_MAX_WORKERS = 4    # 同時テスト実行数\n",
    "\n",
    "BASELINE_ARGS = SimpleNamespace(\n",
    "    gen_length=GEN_LENGTH,\n",
    "    steps=BASELINE_STEPS,\n",
    "    block_size=BLOCK_SIZE,\n",
    "    use_cache=True,\n",
    "    if_cache_position=True,\n",
    "    threshold=PARALLEL_THRESHOLD,\n",
    ")\n",
    "\n",
    "PAIRSPEC_ARGS = SimpleNamespace(\n",
    "    gen_length=GEN_LENGTH,\n",
    "    steps=BASELINE_STEPS,\n",
    "    block_size=BLOCK_SIZE,\n",
    "    use_cache=True,\n",
    "    if_cache_position=True,\n",
    "    threshold=PARALLEL_THRESHOLD,\n",
    "    draft_model=DRAFT_MODEL_PATH,\n",
    "    draft_device=DRAFT_DEVICE,\n",
    "    draft_depth=PAIRSPEC_DRAFT_DEPTH,\n",
    "    draft_steps=PAIRSPEC_DRAFT_STEPS,\n",
    "    accept_policy=PAIRSPEC_ACCEPT_POLICY,\n",
    "    accept_threshold=PAIRSPEC_ACCEPT_THRESHOLD,\n",
    ")\n",
    "\n",
    "print('設定完了: main model =', MAIN_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f889d894",
   "metadata": {},
   "source": [
    "## 5.1 モデルパスの解決（ローカル優先）\n",
    "- 指定ディレクトリが存在しない場合は Hugging Face から Drive に自動ダウンロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1187e3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "LOCAL_MODEL_CACHE_ROOT = '/content/drive/MyDrive/hf_models'\n",
    "\n",
    "def ensure_local_model(identifier: str, cache_root: str = LOCAL_MODEL_CACHE_ROOT):\n",
    "    if identifier is None or identifier == '':\n",
    "        return None\n",
    "    if os.path.isdir(identifier):\n",
    "        print(f\"ローカルパスを使用: {identifier}\")\n",
    "        return identifier\n",
    "    safe_name = identifier.replace('/', '__')\n",
    "    target_dir = os.path.join(cache_root, safe_name)\n",
    "    if os.path.isdir(target_dir):\n",
    "        print(f\"既存のキャッシュを使用: {target_dir}\")\n",
    "        return target_dir\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    print(f\"Hugging Face から {identifier} をダウンロード -> {target_dir}\")\n",
    "    snapshot_download(repo_id=identifier, local_dir=target_dir, local_dir_use_symlinks=False)\n",
    "    return target_dir\n",
    "\n",
    "MAIN_MODEL_PATH_RESOLVED = ensure_local_model(MAIN_MODEL_PATH)\n",
    "DRAFT_MODEL_PATH_RESOLVED = ensure_local_model(DRAFT_MODEL_PATH)\n",
    "PAIRSPEC_ARGS.draft_model = DRAFT_MODEL_PATH_RESOLVED\n",
    "print('Main model path:', MAIN_MODEL_PATH_RESOLVED)\n",
    "print('Draft model path:', DRAFT_MODEL_PATH_RESOLVED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a15f81",
   "metadata": {},
   "source": [
    "## 6. モデルとトークナイザの読み込み\n",
    "- Drive 上にあるモデルフォルダを `from_pretrained` で直接指定します。\n",
    "- bfloat16 / eval モードで読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec70e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from llada.model.modeling_llada import LLaDAModelLM\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if str(device) != VERIFY_DEVICE:\n",
    "    print(f\"Warning: 要求された VERIFY_DEVICE={VERIFY_DEVICE} と実際の device={device} が異なります。\")\n",
    "\n",
    "MODEL_LOAD_PATH = MAIN_MODEL_PATH_RESOLVED or MAIN_MODEL_PATH\n",
    "if MODEL_LOAD_PATH is None:\n",
    "    raise ValueError(\"MAIN_MODEL_PATH を設定してください。\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_LOAD_PATH, trust_remote_code=True)\n",
    "model = LLaDAModelLM.from_pretrained(\n",
    "    MODEL_LOAD_PATH,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").to(device).eval()\n",
    "\n",
    "print('モデル / トークナイザ読み込み完了')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ヘルパー関数の定義\n",
    "- プロンプト生成、最終数値抽出、PairSpec 生成クラス、評価ループなどを実装します。\n",
    "- PairSpec 生成は `specdraft` モジュールを直接利用し、ドラフトワーカを使い回せるようクラス化しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f763b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from llada.chat import _select_generator_name, _select_generator_fn\n",
    "from specdraft.dispatcher import DraftRequest, start_draft_worker, shutdown_draft_worker\n",
    "from specdraft.acceptor import compute_prefix_hash, verify_and_commit\n",
    "from specdraft.kv_manager import KVManager\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def strip_code_fence(text: str) -> str:\n",
    "    stripped = text.strip()\n",
    "    if stripped.startswith(\"```\"):\n",
    "        lines = stripped.splitlines()\n",
    "        if lines and lines[0].startswith(\"```\"):\n",
    "            lines = lines[1:]\n",
    "        while lines and lines[-1].strip() == \"```\":\n",
    "            lines = lines[:-1]\n",
    "        stripped = \"\\n\".join(lines)\n",
    "    return stripped.strip()\n",
    "\n",
    "\n",
    "\n",
    "MASK_TOKEN_ID = tokenizer.mask_token_id or 126336\n",
    "\n",
    "def apply_chat_template(user_text: str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": user_text.strip()},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "def build_gsm8k_prompt(question: str) -> str:\n",
    "    instruction = (\n",
    "        \"Solve the math word problem step by step and output the final numeric answer in the format '#### value'.\\n\\n\"\n",
    "        f\"Problem: {question.strip()}\\n\\nAnswer:\"\n",
    "    )\n",
    "    return apply_chat_template(instruction)\n",
    "\n",
    "def build_humaneval_prompt(sample: dict) -> str:\n",
    "    instruction = (\n",
    "        \"Complete the Python function specification below. Return only executable Python code.\\n\\n\"\n",
    "        f\"{sample['prompt'].strip()}\"\n",
    "    )\n",
    "    return apply_chat_template(instruction)\n",
    "\n",
    "def extract_gsm8k_answer(text: str):\n",
    "    matches = re.findall(r\"####\\s*([-+]?\\d+(?:\\.\\d+)?)\", text)\n",
    "    if matches:\n",
    "        try:\n",
    "            return float(matches[-1])\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def count_new_tokens(tokens: torch.Tensor) -> int:\n",
    "    if tokens is None or tokens.numel() == 0:\n",
    "        return 0\n",
    "    return tokens.shape[1]\n",
    "\n",
    "def run_baseline_generation(prompt_ids: torch.Tensor, args) -> tuple:\n",
    "    generator_name = _select_generator_name(args)\n",
    "    generator_fn = _select_generator_fn(generator_name)\n",
    "    out, nfe = generator_fn(\n",
    "        model,\n",
    "        prompt_ids,\n",
    "        steps=args.steps,\n",
    "        gen_length=args.gen_length,\n",
    "        block_length=args.block_size,\n",
    "        temperature=0.0,\n",
    "        remasking='low_confidence',\n",
    "        threshold=args.threshold,\n",
    "    )\n",
    "    new_tokens = out[:, prompt_ids.shape[1]:]\n",
    "    stats = {\n",
    "        'accepted_blocks': 0,\n",
    "        'attempted_blocks': args.gen_length // args.block_size,\n",
    "        'draft_nfe': 0,\n",
    "        'verify_nfe': 0,\n",
    "        'fallback_nfe': nfe,\n",
    "        'total_nfe': nfe,\n",
    "    }\n",
    "    return new_tokens, nfe, stats\n",
    "\n",
    "def run_block_generation(current_prompt: torch.Tensor, args, steps_per_block: int, generator_name: str):\n",
    "    generator_fn = _select_generator_fn(generator_name)\n",
    "    out, nfe = generator_fn(\n",
    "        model,\n",
    "        current_prompt,\n",
    "        steps=steps_per_block,\n",
    "        gen_length=args.block_size,\n",
    "        block_length=args.block_size,\n",
    "        temperature=0.0,\n",
    "        remasking='low_confidence',\n",
    "        threshold=args.threshold,\n",
    "    )\n",
    "    block_tensor = out[:, -args.block_size:]\n",
    "    return block_tensor, int(nfe)\n",
    "\n",
    "class PairSpecSession:\n",
    "    def __init__(self, args, tokenizer):\n",
    "        self.args = args\n",
    "        self.tokenizer = tokenizer\n",
    "        self.generator_name = _select_generator_name(args)\n",
    "        self.worker, self.request_queue, self.spec_queue = start_draft_worker(\n",
    "            backend='llada',\n",
    "            model_path=args.draft_model,\n",
    "            device=args.draft_device,\n",
    "            generator=self.generator_name,\n",
    "            max_depth=args.draft_depth,\n",
    "            dtype=torch.bfloat16,\n",
    "        )\n",
    "\n",
    "    def close(self):\n",
    "        shutdown_draft_worker(self.worker, self.request_queue)\n",
    "        if self.spec_queue:\n",
    "            self.spec_queue.close()\n",
    "\n",
    "    def generate(self, prompt_ids: torch.Tensor):\n",
    "        args = self.args\n",
    "        block_size = args.block_size\n",
    "        gen_length = args.gen_length\n",
    "        if gen_length % block_size != 0:\n",
    "            raise ValueError('gen_length は block_size で割り切れる必要があります')\n",
    "        total_blocks = gen_length // block_size\n",
    "        steps_per_block = max(1, args.steps // max(total_blocks, 1))\n",
    "        kv_mgr = KVManager(model)\n",
    "        prefix_hash = compute_prefix_hash(prompt_ids[0].tolist())\n",
    "        stats = {\n",
    "            'accepted_blocks': 0,\n",
    "            'attempted_blocks': total_blocks,\n",
    "            'draft_nfe': 0,\n",
    "            'verify_nfe': 0,\n",
    "            'fallback_nfe': 0,\n",
    "        }\n",
    "        pending_blocks = set()\n",
    "\n",
    "        def enqueue(block_id: int, current_prompt: torch.Tensor, current_hash: str):\n",
    "            if block_id >= total_blocks or block_id in pending_blocks:\n",
    "                return\n",
    "            req = DraftRequest(\n",
    "                block_id=block_id,\n",
    "                prefix_tokens=current_prompt[0].tolist(),\n",
    "                prefix_hash=current_hash,\n",
    "                block_size=block_size,\n",
    "                steps_per_block=max(1, args.draft_steps or steps_per_block),\n",
    "                temperature=0.0,\n",
    "                remasking='low_confidence',\n",
    "                threshold=args.threshold,\n",
    "                generator=self.generator_name,\n",
    "            )\n",
    "            self.request_queue.put(req)\n",
    "            pending_blocks.add(block_id)\n",
    "\n",
    "        enqueue(0, prompt_ids, prefix_hash)\n",
    "        current_prompt = prompt_ids.clone()\n",
    "        generated_segments = []\n",
    "\n",
    "        for block_id in range(total_blocks):\n",
    "            draft = self.spec_queue.try_get(block_id)\n",
    "            if draft is not None:\n",
    "                pending_blocks.discard(block_id)\n",
    "                stats['draft_nfe'] += draft.nfe\n",
    "                if draft.prefix_hash != prefix_hash:\n",
    "                    draft = None\n",
    "\n",
    "            accepted_tensor = None\n",
    "            if draft is not None:\n",
    "                verification = verify_and_commit(\n",
    "                    draft,\n",
    "                    model,\n",
    "                    current_prompt,\n",
    "                    mask_token_id=MASK_TOKEN_ID,\n",
    "                    policy=args.accept_policy,\n",
    "                    threshold=args.accept_threshold,\n",
    "                )\n",
    "                stats['verify_nfe'] += verification.nfe\n",
    "                if verification.accepted:\n",
    "                    stats['accepted_blocks'] += 1\n",
    "                    accepted_tensor = torch.tensor(\n",
    "                        verification.accepted_tokens,\n",
    "                        dtype=current_prompt.dtype,\n",
    "                        device=current_prompt.device,\n",
    "                    ).unsqueeze(0)\n",
    "\n",
    "            if accepted_tensor is None:\n",
    "                block_tensor, nfe_block = run_block_generation(\n",
    "                    current_prompt,\n",
    "                    args,\n",
    "                    steps_per_block,\n",
    "                    self.generator_name,\n",
    "                )\n",
    "                stats['fallback_nfe'] += nfe_block\n",
    "            else:\n",
    "                block_tensor = accepted_tensor\n",
    "\n",
    "            current_prompt = torch.cat([current_prompt, block_tensor], dim=1)\n",
    "            prefix_hash = compute_prefix_hash(current_prompt[0].tolist())\n",
    "            kv_mgr.recompute_on_commit(prefix_hash)\n",
    "            generated_segments.append(block_tensor)\n",
    "            enqueue(block_id + 1, current_prompt, prefix_hash)\n",
    "\n",
    "        full_generation = torch.cat(generated_segments, dim=1) if generated_segments else torch.empty((1, 0), dtype=current_prompt.dtype, device=current_prompt.device)\n",
    "        stats['total_nfe'] = stats['verify_nfe'] + stats['fallback_nfe']\n",
    "        return full_generation, stats['total_nfe'], stats\n",
    "\n",
    "def generate_text(prompt_text: str, mode: str, pair_session: PairSpecSession = None):\n",
    "    encoded = tokenizer(prompt_text, return_tensors='pt').input_ids.to(device)\n",
    "    start = time.perf_counter()\n",
    "    if mode == 'pairspec':\n",
    "        if pair_session is None:\n",
    "            raise ValueError('PairSpec モードには PairSpecSession が必要です')\n",
    "        new_tokens, nfe, stats = pair_session.generate(encoded)\n",
    "    else:\n",
    "        new_tokens, nfe, stats = run_baseline_generation(encoded, BASELINE_ARGS)\n",
    "    elapsed = time.perf_counter() - start\n",
    "    decoded = tokenizer.batch_decode(new_tokens, skip_special_tokens=True)[0].strip()\n",
    "    token_count = count_new_tokens(new_tokens)\n",
    "    tps = token_count / elapsed if elapsed > 0 else float('inf')\n",
    "    metrics = {\n",
    "        'latency': elapsed,\n",
    "        'tokens': token_count,\n",
    "        'tps': tps,\n",
    "        'nfe': nfe,\n",
    "        **stats,\n",
    "    }\n",
    "    return decoded, metrics\n",
    "\n",
    "print('ヘルパー関数を定義しました。')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929aa3e9",
   "metadata": {},
   "source": [
    "## 8. 評価ルーチン\n",
    "- GSM8K: 最終数値一致で正解判定。\n",
    "- HumanEval: Hugging Face `code_eval` を用いて pass@1 を算出。\n",
    "- 速度指標はサンプルごとのログから集計します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e662e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "def evaluate_gsm8k(mode: str, max_samples: int = None, pair_session: PairSpecSession = None):\n",
    "    dataset = load_dataset('openai/gsm8k', 'main', split='test')\n",
    "    if max_samples is not None:\n",
    "        dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
    "    records = []\n",
    "    for idx, sample in enumerate(tqdm(dataset, desc=f'GSM8K [{mode}]')):\n",
    "        prompt = build_gsm8k_prompt(sample['question'])\n",
    "        completion, metrics = generate_text(prompt, mode, pair_session)\n",
    "        pred = extract_gsm8k_answer(completion)\n",
    "        gold = extract_gsm8k_answer(sample['answer'])\n",
    "        correct = int(pred is not None and gold is not None and math.isclose(pred, gold))\n",
    "        records.append({\n",
    "            'dataset': 'gsm8k',\n",
    "            'mode': mode,\n",
    "            'index': idx,\n",
    "            'question': sample['question'],\n",
    "            'gold_answer': gold,\n",
    "            'prediction_text': completion,\n",
    "            'prediction_value': pred,\n",
    "            'correct': correct,\n",
    "            **metrics,\n",
    "        })\n",
    "    df = pd.DataFrame(records)\n",
    "    summary = {\n",
    "        'dataset': 'gsm8k',\n",
    "        'mode': mode,\n",
    "        'samples': len(df),\n",
    "        'accuracy': df['correct'].mean() if len(df) else float('nan'),\n",
    "        'avg_latency': df['latency'].mean() if len(df) else float('nan'),\n",
    "        'avg_tps': df['tps'].mean() if len(df) else float('nan'),\n",
    "        'avg_nfe': df['nfe'].mean() if len(df) else float('nan'),\n",
    "        'accepted_block_ratio': (df['accepted_blocks'] / df['attempted_blocks']).mean() if 'accepted_blocks' in df else 0.0,\n",
    "    }\n",
    "    return df, summary\n",
    "\n",
    "def evaluate_humaneval(mode: str, max_samples: int = None, pair_session: PairSpecSession = None):\n",
    "    dataset = load_dataset('openai/openai_humaneval', split='test')\n",
    "    if max_samples is not None:\n",
    "        dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
    "    records = []\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for idx, sample in enumerate(tqdm(dataset, desc=f'HumanEval [{mode}]')):\n",
    "        prompt = build_humaneval_prompt(sample)\n",
    "        completion, metrics = generate_text(prompt, mode, pair_session)\n",
    "        clean_completion = strip_code_fence(completion)\n",
    "        predictions.append(json.dumps({\n",
    "            'task_id': sample['task_id'],\n",
    "            'completion': clean_completion,\n",
    "        }))\n",
    "        references.append(json.dumps({\n",
    "            'task_id': sample['task_id'],\n",
    "            'prompt': sample['prompt'],\n",
    "            'canonical_solution': sample.get('canonical_solution', ''),\n",
    "            'test': sample['test'],\n",
    "            'entry_point': sample['entry_point'],\n",
    "        }))\n",
    "        records.append({\n",
    "            'dataset': 'humaneval',\n",
    "            'mode': mode,\n",
    "            'task_id': sample['task_id'],\n",
    "            'prompt': sample['prompt'],\n",
    "            'completion': clean_completion,\n",
    "            **metrics,\n",
    "        })\n",
    "\n",
    "    code_eval = evaluate.load('code_eval')\n",
    "    eval_result = code_eval.compute(\n",
    "        references=references,\n",
    "        predictions=predictions,\n",
    "        k=[1],\n",
    "        timeout=HUMAN_EVAL_TIMEOUT,\n",
    "    )\n",
    "    pass_dict = eval_result.get('pass@k') or {}\n",
    "    pass_at_1 = pass_dict.get(1, eval_result.get('pass@1', float('nan')))\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    summary = {\n",
    "        'dataset': 'humaneval',\n",
    "        'mode': mode,\n",
    "        'samples': len(df),\n",
    "        'pass@1': pass_at_1,\n",
    "        'avg_latency': df['latency'].mean() if len(df) else float('nan'),\n",
    "        'avg_tps': df['tps'].mean() if len(df) else float('nan'),\n",
    "        'avg_nfe': df['nfe'].mean() if len(df) else float('nan'),\n",
    "        'accepted_block_ratio': (df['accepted_blocks'] / df['attempted_blocks']).mean() if 'accepted_blocks' in df else 0.0,\n",
    "    }\n",
    "    return df, summary, eval_result\n",
    "\n",
    "print('Evaluation routines ready.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ベースライン & PairSpec 評価を実行\n",
    "- それぞれのデータセットについて、PairSpec セッションを初期化してから計測します。\n",
    "- 実行には時間がかかるので、`MAX_SAMPLES` を調整しながら進めてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "summaries = []\n",
    "\n",
    "# --- ベースライン ---\n",
    "gsm8k_baseline_df, gsm8k_baseline_summary = evaluate_gsm8k('baseline', GSM8K_MAX_SAMPLES)\n",
    "all_dfs.append(gsm8k_baseline_df)\n",
    "summaries.append(gsm8k_baseline_summary)\n",
    "\n",
    "humaneval_baseline_df, humaneval_baseline_summary, humaneval_baseline_eval = evaluate_humaneval('baseline', HUMAN_MAX_SAMPLES)\n",
    "all_dfs.append(humaneval_baseline_df)\n",
    "summaries.append(humaneval_baseline_summary)\n",
    "\n",
    "# --- PairSpec ---\n",
    "pairspec_session = PairSpecSession(PAIRSPEC_ARGS, tokenizer)\n",
    "try:\n",
    "    gsm8k_pairspec_df, gsm8k_pairspec_summary = evaluate_gsm8k('pairspec', GSM8K_MAX_SAMPLES, pairspec_session)\n",
    "    all_dfs.append(gsm8k_pairspec_df)\n",
    "    summaries.append(gsm8k_pairspec_summary)\n",
    "\n",
    "    humaneval_pairspec_df, humaneval_pairspec_summary, humaneval_pairspec_eval = evaluate_humaneval('pairspec', HUMAN_MAX_SAMPLES, pairspec_session)\n",
    "    all_dfs.append(humaneval_pairspec_df)\n",
    "    summaries.append(humaneval_pairspec_summary)\n",
    "finally:\n",
    "    pairspec_session.close()\n",
    "\n",
    "full_results_df = pd.concat(all_dfs, ignore_index=True)\n",
    "summary_df = pd.DataFrame(summaries)\n",
    "print('評価完了')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 指標サマリと詳細ログ\n",
    "- `summary_df` でデータセット毎の主要指標を確認できます。\n",
    "- `full_results_df` をフィルタすればサンプルごとのログ（正誤・速度・PairSpec 受理率など）を参照できます。\n",
    "- 必要に応じて CSV で Drive に保存してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例: 先頭5件だけ表示\n",
    "full_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任意: CSV として Drive に保存\n",
    "# summary_df.to_csv('/content/drive/MyDrive/pairspec_eval_summary.csv', index=False)\n",
    "# full_results_df.to_csv('/content/drive/MyDrive/pairspec_eval_details.csv', index=False)\n",
    "print('必要に応じて CSV 保存コマンドを有効化してください。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 追加メモ\n",
    "- HumanEval の `code_eval` は Python コードを実行するため、信頼できる環境でのみ実行してください。\n",
    "- PairSpec のドラフト/検証を単一 GPU で同時に走らせるとメモリ使用量が増えるため、必要に応じて `GEN_LENGTH` や `BLOCK_SIZE`、`draft_depth` を調整してください。\n",
    "- `PAIRSPEC_ARGS` の `accept_policy='thresholded'` に切り替えると、ドラフト受理を信頼度しきい値ベースに変更できます。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}